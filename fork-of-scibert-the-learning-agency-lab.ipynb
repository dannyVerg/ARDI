{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":8950798,"datasetId":5386567,"databundleVersionId":9114078}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install transformers torch scikit-learn pandas","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install chardet\n!pip install seqeval -q\n!pip install nltk","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:50:06.869913Z","iopub.execute_input":"2024-07-14T11:50:06.870708Z","iopub.status.idle":"2024-07-14T11:50:47.660382Z","shell.execute_reply.started":"2024-07-14T11:50:06.870674Z","shell.execute_reply":"2024-07-14T11:50:47.659371Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting chardet\n  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\nDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: chardet\nSuccessfully installed chardet-5.2.0\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import the libraries / Modules","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n#import os\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport gc\nimport json\nimport os\nimport random\nimport re\nimport warnings\nfrom collections import defaultdict\nfrom functools import partial\nfrom typing import Dict\n\nfrom sklearn.metrics import classification_report\nfrom seqeval.metrics import f1_score, precision_score, recall_score\n#from transformers import BertTokenizer, BertForSequenceClassification, AdamW, AutoTokenizer\nfrom transformers import (\n    BertTokenizer,\n    BertForSequenceClassification,\n    AdamW,\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    DataCollatorForTokenClassification,\n    Trainer,\n    TrainingArguments\n)\nimport torch\nfrom datasets import Dataset as HF_dataset\n#from torch.utils.data import DataLoader, Dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:51:20.360831Z","iopub.execute_input":"2024-07-14T11:51:20.361502Z","iopub.status.idle":"2024-07-14T11:51:20.368051Z","shell.execute_reply.started":"2024-07-14T11:51:20.361468Z","shell.execute_reply":"2024-07-14T11:51:20.366946Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Set CFG and Seed","metadata":{}},{"cell_type":"code","source":"all_labels = [\n    'Task',\n    'Dataset',\n    'Metric',\n    'Score',\n    \"O\"\n]\nlabel2id = {l: i for i, l in enumerate(all_labels)}\nid2label = {v: k for k, v in label2id.items()}\n\nprint(id2label)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:51:22.923710Z","iopub.execute_input":"2024-07-14T11:51:22.924734Z","iopub.status.idle":"2024-07-14T11:51:22.932109Z","shell.execute_reply.started":"2024-07-14T11:51:22.924687Z","shell.execute_reply":"2024-07-14T11:51:22.931087Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{0: 'Task', 1: 'Dataset', 2: 'Metric', 3: 'Score', 4: 'O'}\n","output_type":"stream"}]},{"cell_type":"code","source":"class Config:\n    # debug\n    debug = False\n\n    # cross validation\n    do_cv = True\n    fold = 0\n    n_splits = 2 if debug else 4\n\n    # gpu\n    gpu = torch.cuda.is_available()\n\n    # seed\n    seed = 42\n\n    # negative sample frac\n    neg_frac = 1 #0.3\n\n    # external dataset\n#     external_name = \"tonyarobertson\"\n    external_name = \"Last_epoch\" # \n#     external_name = \"mpware\"\n#     external_name = \"valentin\"\n#     external_name = \"moth\"\n#     external_name = \"pjmathematician\"\n    \n    #TODO adjust folders\n    # directory path\n    input_dir = \"/kaggle/working\"\n    comp_dir = input_dir + \"comp_dir\"\n    fold_dir = input_dir + \"sota/dataset/train/\"\n    external_dir = input_dir + \"external_dir\"\n    output_dir = \"/kaggle/working/output/\"\n\n    # file path\n    comp_path = comp_dir + \"train.json\"\n    external_path = external_dir + \"datamix.json\"\n\n    # tokenizer\n    train_max_length = 512 #1536\n    eval_max_length = 512 #3500\n    train_stride = None\n    eval_stride = 256\n\n    # model\n    model_name = \"allenai/scibert_scivocab_uncased\" #ALSO cased scibert\n#     model_name = \"microsoft/deberta-v3-base\"\n#     model_name = \"microsoft/deberta-v3-large\"\n    num_train_epochs = 1 if debug else 3\n    max_steps = 5 if debug else 3000\n    fp16 = True if gpu else False\n    per_device_train_batch_size = 1 \n    gradient_accumulation_steps = 2 \n    learning_rate = 2e-5\n    warmup_ratio = 0.1\n    weight_decay = 0.01\n\n    # postprocessing\n    threshold = 0.95\n\n    # save path\n    if train_stride is not None:\n        save_path = f\"{model_name.split('/')[-1]}-{external_name}-{train_max_length}-{train_stride}-{seed}\"\n    else:\n        save_path = f\"{model_name.split('/')[-1]}-{external_name}-{train_max_length}-{seed}\"\n    if do_cv:\n        save_path = f\"{save_path}-{fold}\"\n\ndef fix_seed(seed):\n    # basic\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n\n    # torch\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nfix_seed(Config.seed)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:53:42.574057Z","iopub.execute_input":"2024-07-14T11:53:42.574902Z","iopub.status.idle":"2024-07-14T11:53:42.637749Z","shell.execute_reply.started":"2024-07-14T11:53:42.574870Z","shell.execute_reply":"2024-07-14T11:53:42.636732Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# load data","metadata":{}},{"cell_type":"code","source":"train_df.to_pickle(\"train_preprocessed_v2\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T09:52:24.368063Z","iopub.execute_input":"2024-07-14T09:52:24.368705Z","iopub.status.idle":"2024-07-14T09:53:17.919201Z","shell.execute_reply.started":"2024-07-14T09:52:24.368665Z","shell.execute_reply":"2024-07-14T09:53:17.918285Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"val_df.to_pickle(\"val_preprocessed_v2\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T09:53:17.920833Z","iopub.execute_input":"2024-07-14T09:53:17.921131Z","iopub.status.idle":"2024-07-14T09:53:18.214539Z","shell.execute_reply.started":"2024-07-14T09:53:17.921105Z","shell.execute_reply":"2024-07-14T09:53:18.213719Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Example usage with raw string for folder_path\n\ntrain_df = pd.read_pickle('/kaggle/input/sota-preprocessed-v3/train_preprocessed_v2')\n\ntrain_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:51:43.957488Z","iopub.execute_input":"2024-07-14T11:51:43.958406Z","iopub.status.idle":"2024-07-14T11:52:57.135044Z","shell.execute_reply.started":"2024-07-14T11:51:43.958371Z","shell.execute_reply":"2024-07-14T11:52:57.134113Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"       document                                          full_text  \\\n0    1501.07800  \\documentclass{elsarticle}\\n\\usepackage{color,...   \n1    1509.04927  \\documentclass[12pt,twoside,a4paper]{article}\\...   \n2  2204.01208v1  \\begin{filecontents*}{example.eps}\\ngsave\\nnew...   \n\n                                            labels_1  \\\n0                                               None   \n1                                               None   \n2  [{'LEADERBOARD': {'Task': 'GZSL Video Classifi...   \n\n                                              tokens  \\\n0  [\\documentclass, {, elsarticle, }, \\usepackage...   \n1  [\\documentclass, [, 12pt, ,, twoside, ,, a4pap...   \n2  [\\begin, {, filecontents, *, }, {, example.eps...   \n\n                                 trailing_whitespace  \\\n0  [False, False, False, True, False, False, Fals...   \n1  [False, False, False, False, False, False, Fal...   \n2  [False, False, False, False, False, False, Fal...   \n\n                                        token_labels  \\\n0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n2  [O, O, O, O, O, O, O, O, O, O, Score, Score, O...   \n\n                                     provided_labels  \\\n0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n2  [O, O, O, O, O, O, O, O, O, O, Score, Score, O...   \n\n                                         word_labels  \\\n0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n\n                                           input_ids  \\\n0  [102, 4088, 3367, 5717, 1342, 847, 22851, 2664...   \n1  [102, 4088, 3367, 5717, 260, 760, 489, 422, 50...   \n2  [102, 4088, 3973, 1342, 4433, 13741, 30113, 13...   \n\n                                      token_type_ids  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                      attention_mask  \\\n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n\n                                      offset_mapping  \\\n0  [(0, 0), (0, 1), (1, 9), (9, 14), (14, 15), (1...   \n1  [(0, 0), (0, 1), (1, 9), (9, 14), (14, 15), (1...   \n2  [(0, 0), (0, 1), (1, 6), (6, 7), (7, 11), (11,...   \n\n                                              labels  length  \n0  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...     512  \n1  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...     512  \n2  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...     512  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>full_text</th>\n      <th>labels_1</th>\n      <th>tokens</th>\n      <th>trailing_whitespace</th>\n      <th>token_labels</th>\n      <th>provided_labels</th>\n      <th>word_labels</th>\n      <th>input_ids</th>\n      <th>token_type_ids</th>\n      <th>attention_mask</th>\n      <th>offset_mapping</th>\n      <th>labels</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1501.07800</td>\n      <td>\\documentclass{elsarticle}\\n\\usepackage{color,...</td>\n      <td>None</td>\n      <td>[\\documentclass, {, elsarticle, }, \\usepackage...</td>\n      <td>[False, False, False, True, False, False, Fals...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[102, 4088, 3367, 5717, 1342, 847, 22851, 2664...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[(0, 0), (0, 1), (1, 9), (9, 14), (14, 15), (1...</td>\n      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n      <td>512</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1509.04927</td>\n      <td>\\documentclass[12pt,twoside,a4paper]{article}\\...</td>\n      <td>None</td>\n      <td>[\\documentclass, [, 12pt, ,, twoside, ,, a4pap...</td>\n      <td>[False, False, False, False, False, False, Fal...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[102, 4088, 3367, 5717, 260, 760, 489, 422, 50...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[(0, 0), (0, 1), (1, 9), (9, 14), (14, 15), (1...</td>\n      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n      <td>512</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2204.01208v1</td>\n      <td>\\begin{filecontents*}{example.eps}\\ngsave\\nnew...</td>\n      <td>[{'LEADERBOARD': {'Task': 'GZSL Video Classifi...</td>\n      <td>[\\begin, {, filecontents, *, }, {, example.eps...</td>\n      <td>[False, False, False, False, False, False, Fal...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, Score, Score, O...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, Score, Score, O...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[102, 4088, 3973, 1342, 4433, 13741, 30113, 13...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[(0, 0), (0, 1), (1, 6), (6, 7), (7, 11), (11,...</td>\n      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n      <td>512</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"val_df = pd.read_pickle('/kaggle/input/sota-preprocessed-v3/val_preprocessed_v2')","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:53:02.422416Z","iopub.execute_input":"2024-07-14T11:53:02.423097Z","iopub.status.idle":"2024-07-14T11:53:02.991327Z","shell.execute_reply.started":"2024-07-14T11:53:02.423067Z","shell.execute_reply":"2024-07-14T11:53:02.990554Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\ndata_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\nargs = TrainingArguments(\n    output_dir=Config.output_dir, \n    fp16=Config.fp16,\n    per_device_train_batch_size=Config.per_device_train_batch_size,\n    gradient_accumulation_steps=Config.gradient_accumulation_steps,\n    num_train_epochs=Config.num_train_epochs,\n#     max_steps=Config.max_steps,\n    learning_rate=Config.learning_rate,\n    warmup_ratio=Config.warmup_ratio,\n    weight_decay=Config.weight_decay,\n#     group_by_length=True,\n    #evaluation_strategy=\"no\",\n    evaluation_strategy='steps',\n    save_strategy='steps',\n    eval_steps=5 if Config.debug else 100,\n    save_steps=5 if Config.debug else 100,\n    logging_steps=0.05,\n    #save_strategy=\"no\",\n    save_total_limit=300,\n    lr_scheduler_type=\"cosine\",\n    metric_for_best_model=\"f1yue\",\n    load_best_model_at_end=True,\n    report_to=\"none\",\n    seed=Config.seed,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:53:49.290865Z","iopub.execute_input":"2024-07-14T11:53:49.291645Z","iopub.status.idle":"2024-07-14T11:53:50.397457Z","shell.execute_reply.started":"2024-07-14T11:53:49.291612Z","shell.execute_reply":"2024-07-14T11:53:50.396472Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c541caefff74f72bd7d638d634e7a06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff7e9a2a7df0404ab03d8355a5bda2a5"}},"metadata":{}}]},{"cell_type":"code","source":"def freeze(module):\n    for parameter in module.parameters():\n        parameter.requires_grad = False\n\n\ndef compute_metrics(res, all_labels):\n    predictions, labels = res\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    recall = recall_score(true_labels, true_predictions)\n    precision = precision_score(true_labels, true_predictions)\n    f1_score = (1 + 1) * recall * precision / (1 * precision + recall)\n\n    return {\"recall\": recall, \"precision\": precision, \"f1yue\": f1_score}\n\n\ndef train(model_name, all_labels, id2label, label2id, ds, eval_ds, args, data_collator, tokenizer, save_path):\n    model = AutoModelForTokenClassification.from_pretrained(\n        model_name,\n        num_labels=len(all_labels),\n        id2label=id2label,\n        label2id=label2id,\n        ignore_mismatched_sizes=True,\n    )\n\n    # update initial weight （default: mean=0.0, std=0.02）\n    model.classifier.weight.data.normal_(mean=0.0, std=0.01)\n\n    #if model_name == \"microsoft/deberta-v3-large\":\n    #    # freezing embeddings and first 4 layers of encoder\n    #    freeze(model.deberta.embeddings)\n    #    for i, layer in enumerate(model.deberta.encoder.layer[:4]):\n    #        print(f\"freeze layer {i+1} of encoder\")\n    #        freeze(layer)\n\n    trainer = Trainer(\n        model=model, \n        args=args, \n        train_dataset=ds,\n        eval_dataset=eval_ds,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n        compute_metrics=partial(compute_metrics, all_labels=all_labels)\n    )\n    trainer.train()\n    trainer.save_model(save_path)\n\n    del model, trainer\n    torch.cuda.empty_cache()\n    _ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:53:56.990444Z","iopub.execute_input":"2024-07-14T11:53:56.991511Z","iopub.status.idle":"2024-07-14T11:53:57.004439Z","shell.execute_reply.started":"2024-07-14T11:53:56.991464Z","shell.execute_reply":"2024-07-14T11:53:57.003299Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"ds = HF_dataset.from_pandas(train_df, preserve_index=False)\nds_val = HF_dataset.from_pandas(val_df, preserve_index=False)\n\ntrain(\n    Config.model_name,\n    all_labels,\n    id2label,\n    label2id,\n    ds,\n    ds_val,\n    args,\n    data_collator,\n    tokenizer,\n    Config.save_path,\n)\n\ntokenizer.save_pretrained(Config.save_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:54:01.771032Z","iopub.execute_input":"2024-07-14T11:54:01.771383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -tl output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DONE! If you don't want to use Trainer from Huggingface, you need to do it by your own.","metadata":{}},{"cell_type":"markdown","source":"## (Load data)","metadata":{}},{"cell_type":"code","source":"# pd.read_json('/kaggle/input/pii-detection-removal-from-educational-data/train.json').shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into train and validation sets\n#train_df, val_df = train_test_split(df_train, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (Build dataset)","metadata":{}},{"cell_type":"code","source":"'''class CustomDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        text = self.dataframe.iloc[idx]['full_text']\n        label = self.dataframe.iloc[idx]['document']\n        encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, padding='max_length', return_tensors='pt')\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n        }'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the BERT tokenizer\n#tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''# Define the maximum sequence length\nmax_length = 128\n# batch size\nbatch_size = 16'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_df.iloc[4750].full_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizer(train_df.iloc[4750].tokens,add_special_tokens=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizer.tokenize(train_df.iloc[4750].full_text,)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizer.convert_ids_to_tokens(tokenizer(train_df.iloc[4750].full_text).input_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (Build dataloader)","metadata":{}},{"cell_type":"code","source":"'''# Create instances of the custom dataset class for training and validation\ntrain_dataset = CustomDataset(train_df, tokenizer, max_length)\nval_dataset = CustomDataset(val_df, tokenizer, max_length)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''train_df['document'].max()'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''# Create data loaders for training and validation\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (Build model)","metadata":{}},{"cell_type":"code","source":"'''# Initialize the pre-trained BERT model for sequence classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df_train['document'].unique()))'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (Build optimizer and loss)","metadata":{}},{"cell_type":"code","source":"'''# optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Define the loss function\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index=-1)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (set device)","metadata":{}},{"cell_type":"code","source":"'''# Define the device\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''model.to(device)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (Define training process)","metadata":{}},{"cell_type":"code","source":"'''def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler=None):\n    model.train()\n    losses = []\n    correct_predictions = 0\n\n    for batch in data_loader:  # Iterate over batches in the data loader\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs.logits, dim=1)\n\n        # Use ignore_index to handle potential unseen labels during training\n        loss = loss_fn(outputs.logits, labels, ignore_index=-1)  # Set -1 as ignore_index (optional)\n        correct_predictions += torch.sum(preds == labels)\n        losses.append(loss.item())\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        if scheduler:\n            scheduler.step()\n\n    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (Define Evaliation)","metadata":{}},{"cell_type":"code","source":"'''# Define the evaluation function\ndef eval_model(model, data_loader, loss_fn, device):\n    model.eval()\n    losses = []\n    correct_predictions = 0\n\n    with torch.no_grad():\n        for batch in data_loader:  # Iterate over batches in the data loader\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            _, preds = torch.max(outputs.logits, dim=1)\n\n            loss = loss_fn(outputs.logits, labels)\n            correct_predictions += torch.sum(preds == labels)\n            losses.append(loss.item())\n\n    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import torch.nn.functional as F'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (Train and eval)","metadata":{}},{"cell_type":"code","source":"'''# Training loop\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses = []\n    correct_predictions = 0\n    total_samples = 0\n  \n    for batch in train_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        print(input_ids.shape,labels.shape)\n      \n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask)\n        print(logits.shape)\n        logits = outputs.logits\n      \n        # Calculate the loss\n        loss = F.cross_entropy(logits, labels, ignore_index=-1)\n      \n        loss.backward()\n        optimizer.step()\n      \n        train_losses.append(loss.item())\n        correct_predictions += torch.sum(torch.argmax(logits, dim=1) == labels)\n        total_samples += labels.size(0)\n      \n    train_accuracy = correct_predictions.double() / total_samples\n    train_loss = sum(train_losses) / len(train_losses)\n  \n    print(f'Epoch {epoch + 1}/{num_epochs}, Train Accuracy: {train_accuracy:.4f}, Train Loss: {train_loss:.4f}')'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}