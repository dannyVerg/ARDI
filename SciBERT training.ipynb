{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8950798,"sourceType":"datasetVersion","datasetId":5386567},{"sourceId":8954223,"sourceType":"datasetVersion","datasetId":5388845}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install transformers torch scikit-learn pandas","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install chardet\n!pip install seqeval -q\n!pip install nltk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import the libraries / Modules","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n#import os\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport gc\nimport json\nimport os\nimport random\nimport re\nimport warnings\nfrom collections import defaultdict\nfrom functools import partial\nfrom typing import Dict\n\nfrom sklearn.metrics import classification_report\nfrom seqeval.metrics import f1_score, precision_score, recall_score\n#from transformers import BertTokenizer, BertForSequenceClassification, AdamW, AutoTokenizer\nfrom transformers import (\n    BertTokenizer,\n    BertForSequenceClassification,\n    AdamW,\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    DataCollatorForTokenClassification,\n    Trainer,\n    TrainingArguments\n)\nimport torch\nfrom datasets import Dataset as HF_dataset\n#from torch.utils.data import DataLoader, Dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:24:25.464509Z","iopub.execute_input":"2024-07-14T23:24:25.465669Z","iopub.status.idle":"2024-07-14T23:24:25.47241Z","shell.execute_reply.started":"2024-07-14T23:24:25.465612Z","shell.execute_reply":"2024-07-14T23:24:25.471485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set CFG and Seed","metadata":{}},{"cell_type":"code","source":"all_labels = [\n    \"O\",\n    'Task',\n    'Dataset',\n    'Metric',\n    'Score',\n]\nlabel2id = {l: i for i, l in enumerate(all_labels)}\nid2label = {v: k for k, v in label2id.items()}\n\nprint(id2label)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:24:25.479935Z","iopub.execute_input":"2024-07-14T23:24:25.480468Z","iopub.status.idle":"2024-07-14T23:24:25.486197Z","shell.execute_reply.started":"2024-07-14T23:24:25.48044Z","shell.execute_reply":"2024-07-14T23:24:25.485263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    # debug\n    debug = False\n    is_use_daniia_dataset = False\n\n    # cross validation\n    do_cv = True\n    fold = 0\n    n_splits = 2 if debug else 4\n\n    # gpu\n    gpu = torch.cuda.is_available()\n\n    # seed\n    seed = 42\n\n    # negative sample frac\n    neg_frac = 0.3 #0.3\n\n    # external dataset\n#     external_name = \"tonyarobertson\"\n    external_name = \"Last_epoch\" # \n#     external_name = \"mpware\"\n#     external_name = \"valentin\"\n#     external_name = \"moth\"\n#     external_name = \"pjmathematician\"\n    \n    #TODO adjust folders\n    # directory path\n    input_dir = \"/kaggle/working\"\n    comp_dir = input_dir + \"comp_dir\"\n    fold_dir = input_dir + \"sota/dataset/train/\"\n    external_dir = input_dir + \"external_dir\"\n    output_dir = \"/kaggle/working/output/\"\n\n    # file path\n    comp_path = comp_dir + \"train.json\"\n    external_path = external_dir + \"datamix.json\"\n\n    # tokenizer\n    train_max_length = 512 #1536\n    eval_max_length = 512 #3500\n    train_stride = None\n    eval_stride = 256\n\n    # model\n    model_name = \"allenai/scibert_scivocab_uncased\" #ALSO cased scibert\n#     model_name = \"microsoft/deberta-v3-base\"\n#     model_name = \"microsoft/deberta-v3-large\"\n    num_train_epochs = 1 if debug else 3\n    max_steps = 5 if debug else 3000\n    fp16 = True if gpu else False\n    per_device_train_batch_size = 16\n    gradient_accumulation_steps = 1 \n    learning_rate = 3e-5\n    warmup_ratio = 0.1\n    weight_decay = 0.01\n\n    # postprocessing\n    threshold = 0.95\n\n    # save path\n    if train_stride is not None:\n        save_path = f\"{model_name.split('/')[-1]}-{external_name}-{train_max_length}-{train_stride}-{seed}\"\n    else:\n        save_path = f\"{model_name.split('/')[-1]}-{external_name}-{train_max_length}-{seed}\"\n    if do_cv:\n        save_path = f\"{save_path}-{fold}\"\n\ndef fix_seed(seed):\n    # basic\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n\n    # torch\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nfix_seed(Config.seed)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:24:25.500117Z","iopub.execute_input":"2024-07-14T23:24:25.500899Z","iopub.status.idle":"2024-07-14T23:24:25.512259Z","shell.execute_reply.started":"2024-07-14T23:24:25.500871Z","shell.execute_reply":"2024-07-14T23:24:25.511528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# load data","metadata":{}},{"cell_type":"code","source":"#train_df.to_pickle(\"train_preprocessed_v2\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:24:25.518017Z","iopub.execute_input":"2024-07-14T23:24:25.518342Z","iopub.status.idle":"2024-07-14T23:24:25.523769Z","shell.execute_reply.started":"2024-07-14T23:24:25.518317Z","shell.execute_reply":"2024-07-14T23:24:25.522895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#val_df.to_pickle(\"val_preprocessed_v2\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:24:25.536192Z","iopub.execute_input":"2024-07-14T23:24:25.536715Z","iopub.status.idle":"2024-07-14T23:24:25.540231Z","shell.execute_reply.started":"2024-07-14T23:24:25.536691Z","shell.execute_reply":"2024-07-14T23:24:25.539266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Example usage with raw string for folder_path\n\n#train_df = pd.read_pickle('/kaggle/input/sota-preprocessed-v3/train_preprocessed_v2')[['word_labels','input_ids','token_type_ids','attention_mask','offset_mapping','labels']]\nif Config.is_use_daniia_dataset:\n    train_df = pd.read_pickle('/kaggle/input/sota-preprocessed-v3/train_preprocessed_v2')[['input_ids','labels']]\nelse:\n    train_df = pd.read_csv('/kaggle/input/sota-all-ready/output0_ready.csv')[['input_ids','labels','with_positive']]\n    train_df = pd.concat([train_df,pd.read_csv('/kaggle/input/sota-all-ready/output1_ready.csv')[['input_ids','labels','with_positive']]],ignore_index=True)\n    train_df = pd.concat([train_df,pd.read_csv('/kaggle/input/sota-all-ready/output2_ready.csv')[['input_ids','labels','with_positive']]],ignore_index=True)\n    train_df = pd.concat([train_df,pd.read_csv('/kaggle/input/sota-all-ready/output3_ready.csv')[['input_ids','labels','with_positive']]],ignore_index=True)\n    train_df = pd.concat([train_df,pd.read_csv('/kaggle/input/sota-all-ready/output4_ready.csv')[['input_ids','labels','with_positive']]],ignore_index=True)\n    train_df = pd.concat([train_df,pd.read_csv('/kaggle/input/sota-all-ready/output5_ready.csv')[['input_ids','labels','with_positive']]],ignore_index=True)\n    train_df = pd.concat([train_df,pd.read_csv('/kaggle/input/sota-all-ready/output7_ready.csv')[['input_ids','labels','with_positive']]],ignore_index=True)\n    train_df = pd.concat([train_df,pd.read_csv('/kaggle/input/sota-all-ready/output8_ready.csv')[['input_ids','labels','with_positive']]],ignore_index=True)\n    train_df = pd.concat([train_df,pd.read_csv('/kaggle/input/sota-all-ready/output9_ready.csv')[['input_ids','labels','with_positive']]],ignore_index=True)\n    train_df = pd.concat([train_df,pd.read_csv('/kaggle/input/sota-all-ready/output10_ready.csv')[['input_ids','labels','with_positive']]],ignore_index=True)\n\ntrain_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:24:25.556054Z","iopub.execute_input":"2024-07-14T23:24:25.556333Z","iopub.status.idle":"2024-07-14T23:24:35.74977Z","shell.execute_reply.started":"2024-07-14T23:24:25.556309Z","shell.execute_reply":"2024-07-14T23:24:35.748808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:24:35.751796Z","iopub.execute_input":"2024-07-14T23:24:35.752283Z","iopub.status.idle":"2024-07-14T23:24:35.757976Z","shell.execute_reply.started":"2024-07-14T23:24:35.752246Z","shell.execute_reply":"2024-07-14T23:24:35.757005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Config.neg_frac<1:\n    def get_is_positive(row):\n        #word_labels = []\n        #print(row[\"labels\"])\n        #for t in row[\"labels\"]:\n        #    #print(t,len(t))\n        #    #word_labels.extend([l] * len(t))\n        #    #if ws:\n        #    #    word_labels.append(\"O\")\n        if len(set(row[\"labels\"]))>1:\n            return True\n        else:\n            return False\n        #return word_labels\n    if Config.is_use_daniia_dataset:\n        train_df['with_positive'] = train_df.apply(lambda row: get_is_positive(row), axis=1) # ready_dataset no need\n    train_df_pos = train_df[train_df['with_positive']>0]\n    train_df_neg = train_df[train_df['with_positive']<=0].sample(frac=Config.neg_frac)\n    train_df_new = pd.concat([train_df_pos,train_df_neg],ignore_index=True).drop(columns=\"with_positive\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:24:35.759137Z","iopub.execute_input":"2024-07-14T23:24:35.759492Z","iopub.status.idle":"2024-07-14T23:24:35.80516Z","shell.execute_reply.started":"2024-07-14T23:24:35.759468Z","shell.execute_reply":"2024-07-14T23:24:35.804264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_new.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:24:35.807254Z","iopub.execute_input":"2024-07-14T23:24:35.807547Z","iopub.status.idle":"2024-07-14T23:24:35.81314Z","shell.execute_reply.started":"2024-07-14T23:24:35.807521Z","shell.execute_reply":"2024-07-14T23:24:35.812271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert new str to new list\nif not Config.is_use_daniia_dataset:\n    from ast import literal_eval\n    def convert_str_to_list(row,term):\n        #if len(set(row[\"labels\"]))>1:\n        #    return True\n        #else:\n        #    return False\n        return literal_eval(row[term])\n        #return word_labels\n\n    train_df_new['input_ids'] = train_df_new.apply(lambda row: convert_str_to_list(row,'input_ids'), axis=1)\n    train_df_new['labels'] = train_df_new.apply(lambda row: convert_str_to_list(row,'labels'), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:24:35.814411Z","iopub.execute_input":"2024-07-14T23:24:35.814916Z","iopub.status.idle":"2024-07-14T23:27:20.411917Z","shell.execute_reply.started":"2024-07-14T23:24:35.814891Z","shell.execute_reply":"2024-07-14T23:27:20.411113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Config.is_use_daniia_dataset:\n    val_df = pd.read_pickle('/kaggle/input/sota-preprocessed-v3/val_preprocessed_v2')[['input_ids','labels']]#[['word_labels','input_ids','token_type_ids','attention_mask','offset_mapping','labels']]\nelse:\n    val_df = pd.read_csv('../input/sota-all-ready/output_val_ready.csv')[['input_ids','labels']]\n    val_df['input_ids'] = val_df.apply(lambda row: convert_str_to_list(row,'input_ids'), axis=1)\n    val_df['labels'] = val_df.apply(lambda row: convert_str_to_list(row,'labels'), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:27:20.413023Z","iopub.execute_input":"2024-07-14T23:27:20.413322Z","iopub.status.idle":"2024-07-14T23:27:25.815798Z","shell.execute_reply.started":"2024-07-14T23:27:20.413296Z","shell.execute_reply":"2024-07-14T23:27:25.815004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:27:25.81689Z","iopub.execute_input":"2024-07-14T23:27:25.817176Z","iopub.status.idle":"2024-07-14T23:27:25.849894Z","shell.execute_reply.started":"2024-07-14T23:27:25.81715Z","shell.execute_reply":"2024-07-14T23:27:25.849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_df\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:27:25.850996Z","iopub.execute_input":"2024-07-14T23:27:25.851305Z","iopub.status.idle":"2024-07-14T23:27:26.719082Z","shell.execute_reply.started":"2024-07-14T23:27:25.851281Z","shell.execute_reply":"2024-07-14T23:27:26.718105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# change label to O-0 task-1 dataset-2 metric-3 score-4, when use ready dataset no need\nif Config.is_use_daniia_dataset:\n    def change_label(row):\n        #print(row[\"labels\"],type(row[\"labels\"]))\n\n        #if len(set(row[\"labels\"]))>1:\n        #    return True\n        #else:\n        #    return False\n        #return word_labels\n        new_labels_list = [ii+1 for ii in row[\"labels\"]]\n        new_labels_list = [ii if ii!=5 else 0 for ii in new_labels_list]\n        #print(new_labels_list)\n        #raise ValueError()\n        return new_labels_list\n    train_df_new['labels'] = train_df_new.apply(lambda row: change_label(row), axis=1)\n    val_df['labels'] = val_df.apply(lambda row: change_label(row), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:27:26.720374Z","iopub.execute_input":"2024-07-14T23:27:26.720666Z","iopub.status.idle":"2024-07-14T23:27:26.728957Z","shell.execute_reply.started":"2024-07-14T23:27:26.720641Z","shell.execute_reply":"2024-07-14T23:27:26.728258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\ndata_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\nargs = TrainingArguments(\n    output_dir=Config.output_dir, \n    fp16=Config.fp16,\n    per_device_train_batch_size=Config.per_device_train_batch_size,\n    gradient_accumulation_steps=Config.gradient_accumulation_steps,\n    num_train_epochs=Config.num_train_epochs,\n#     max_steps=Config.max_steps,\n    learning_rate=Config.learning_rate,\n    warmup_ratio=Config.warmup_ratio,\n    weight_decay=Config.weight_decay,\n#     group_by_length=True,\n    #evaluation_strategy=\"no\",\n    evaluation_strategy='steps',\n    save_strategy='steps',\n    eval_steps=5 if Config.debug else 100,\n    save_steps=5 if Config.debug else 100,\n    logging_steps=0.05,\n    #save_strategy=\"no\",\n    save_total_limit=5,\n    lr_scheduler_type=\"cosine\",\n    metric_for_best_model=\"f1yue\",\n    load_best_model_at_end=True,\n    report_to=\"none\",\n    seed=Config.seed,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:27:26.732524Z","iopub.execute_input":"2024-07-14T23:27:26.732864Z","iopub.status.idle":"2024-07-14T23:27:27.180885Z","shell.execute_reply.started":"2024-07-14T23:27:26.732839Z","shell.execute_reply":"2024-07-14T23:27:27.179919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def freeze(module):\n    for parameter in module.parameters():\n        parameter.requires_grad = False\n\n\ndef compute_metrics(res, all_labels):\n    predictions, labels = res\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    recall = recall_score(true_labels, true_predictions)\n    precision = precision_score(true_labels, true_predictions)\n    f1_score = (1 + 1) * recall * precision / (1 * precision + recall)\n\n    return {\"recall\": recall, \"precision\": precision, \"f1yue\": f1_score}\n\n\ndef train(model_name, all_labels, id2label, label2id, ds, eval_ds, args, data_collator, tokenizer, save_path):\n    model = AutoModelForTokenClassification.from_pretrained(\n        model_name,\n        num_labels=len(all_labels),\n        id2label=id2label,\n        label2id=label2id,\n        ignore_mismatched_sizes=True,\n    )\n\n    # update initial weight （default: mean=0.0, std=0.02）\n    model.classifier.weight.data.normal_(mean=0.0, std=0.01)\n\n    #if model_name == \"microsoft/deberta-v3-large\":\n    #    # freezing embeddings and first 4 layers of encoder\n    #    freeze(model.deberta.embeddings)\n    #    for i, layer in enumerate(model.deberta.encoder.layer[:4]):\n    #        print(f\"freeze layer {i+1} of encoder\")\n    #        freeze(layer)\n\n    trainer = Trainer(\n        model=model, \n        args=args, \n        train_dataset=ds,\n        eval_dataset=eval_ds,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n        compute_metrics=partial(compute_metrics, all_labels=all_labels)\n    )\n    trainer.train()\n    trainer.save_model(save_path)\n\n    #del model, trainer\n    torch.cuda.empty_cache()\n    _ = gc.collect()\n    return model, trainer","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:27:27.182195Z","iopub.execute_input":"2024-07-14T23:27:27.182581Z","iopub.status.idle":"2024-07-14T23:27:27.194098Z","shell.execute_reply.started":"2024-07-14T23:27:27.182547Z","shell.execute_reply":"2024-07-14T23:27:27.193261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = HF_dataset.from_pandas(train_df_new, preserve_index=False)\nds_val = HF_dataset.from_pandas(val_df, preserve_index=False)\n\nmodel, trainer = train(\n    Config.model_name,\n    all_labels,\n    id2label,\n    label2id,\n    ds,\n    ds_val,\n    args,\n    data_collator,\n    tokenizer,\n    Config.save_path,\n)\n\ntokenizer.save_pretrained(Config.save_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T23:27:27.195365Z","iopub.execute_input":"2024-07-14T23:27:27.1957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -tl output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# predict","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DONE! If you don't want to use Trainer from Huggingface, you need to do it by your own.","metadata":{}},{"cell_type":"markdown","source":"## (Load data)","metadata":{}},{"cell_type":"code","source":"# pd.read_json('/kaggle/input/pii-detection-removal-from-educational-data/train.json').shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into train and validation sets\n#train_df, val_df = train_test_split(df_train, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (Build dataset)","metadata":{}},{"cell_type":"code","source":"'''class CustomDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        text = self.dataframe.iloc[idx]['full_text']\n        label = self.dataframe.iloc[idx]['document']\n        encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, padding='max_length', return_tensors='pt')\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n        }'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the BERT tokenizer\n#tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''# Define the maximum sequence length\nmax_length = 128\n# batch size\nbatch_size = 16'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_df.iloc[4750].full_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizer(train_df.iloc[4750].tokens,add_special_tokens=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizer.tokenize(train_df.iloc[4750].full_text,)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizer.convert_ids_to_tokens(tokenizer(train_df.iloc[4750].full_text).input_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (Build dataloader)","metadata":{}},{"cell_type":"code","source":"'''# Create instances of the custom dataset class for training and validation\ntrain_dataset = CustomDataset(train_df, tokenizer, max_length)\nval_dataset = CustomDataset(val_df, tokenizer, max_length)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''train_df['document'].max()'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''# Create data loaders for training and validation\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (Build model)","metadata":{}},{"cell_type":"code","source":"'''# Initialize the pre-trained BERT model for sequence classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df_train['document'].unique()))'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (Build optimizer and loss)","metadata":{}},{"cell_type":"code","source":"'''# optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Define the loss function\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index=-1)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (set device)","metadata":{}},{"cell_type":"code","source":"'''# Define the device\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''model.to(device)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (Define training process)","metadata":{}},{"cell_type":"code","source":"'''def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler=None):\n    model.train()\n    losses = []\n    correct_predictions = 0\n\n    for batch in data_loader:  # Iterate over batches in the data loader\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs.logits, dim=1)\n\n        # Use ignore_index to handle potential unseen labels during training\n        loss = loss_fn(outputs.logits, labels, ignore_index=-1)  # Set -1 as ignore_index (optional)\n        correct_predictions += torch.sum(preds == labels)\n        losses.append(loss.item())\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        if scheduler:\n            scheduler.step()\n\n    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (Define Evaliation)","metadata":{}},{"cell_type":"code","source":"'''# Define the evaluation function\ndef eval_model(model, data_loader, loss_fn, device):\n    model.eval()\n    losses = []\n    correct_predictions = 0\n\n    with torch.no_grad():\n        for batch in data_loader:  # Iterate over batches in the data loader\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            _, preds = torch.max(outputs.logits, dim=1)\n\n            loss = loss_fn(outputs.logits, labels)\n            correct_predictions += torch.sum(preds == labels)\n            losses.append(loss.item())\n\n    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import torch.nn.functional as F'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (Train and eval)","metadata":{}},{"cell_type":"code","source":"'''# Training loop\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses = []\n    correct_predictions = 0\n    total_samples = 0\n  \n    for batch in train_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        print(input_ids.shape,labels.shape)\n      \n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask)\n        print(logits.shape)\n        logits = outputs.logits\n      \n        # Calculate the loss\n        loss = F.cross_entropy(logits, labels, ignore_index=-1)\n      \n        loss.backward()\n        optimizer.step()\n      \n        train_losses.append(loss.item())\n        correct_predictions += torch.sum(torch.argmax(logits, dim=1) == labels)\n        total_samples += labels.size(0)\n      \n    train_accuracy = correct_predictions.double() / total_samples\n    train_loss = sum(train_losses) / len(train_losses)\n  \n    print(f'Epoch {epoch + 1}/{num_epochs}, Train Accuracy: {train_accuracy:.4f}, Train Loss: {train_loss:.4f}')'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}